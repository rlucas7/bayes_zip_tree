\documentclass{article}
\usepackage{dsfont, amsmath}

\usepackage{graphics}
\usepackage{epsfig,dsfont}
\usepackage{amsmath, amssymb, hyperref,float}
\usepackage{framed, pseudocode}
\usepackage{mathrsfs} 
\usepackage{graphicx, dsfont}
\usepackage{amsthm,fullpage}
\usepackage{graphicx,float,hyperref}




%%%%----------------------------------------- Code for abbreviation list
% Macro for 'abbreviations' etc...
\def\listofabbreviations{\input{abbreviations} \clearpage}
\def\addabbrev #1: #2#3{#1 \> \parbox{7in}{#2 \dotfill \pageref{#3}}\\} 
\def\newabbrev#1{\label{#1}} 
%%%%%--------------------------------------End of code for notation list


\newtheorem{mydef}{Definition}
\newtheorem{lem}{Lemma}
\newtheorem{thrm}{Theorem}

%\graphicspath{ {/Users/rlucas7/Dropbox/Public/Thesis_Proposal/figures/} }
%\graphicspath{ {images_folder/}{other_folder/}{third_folder/} } use this if there are several filepaths or folders for images 

%for fancy script letters... $\mathscr{ABCDEFGHIJKLMNOPQRSTUVWXYZ}$
\newcommand{\yvec}{\underline{y}}
\newcommand{\xvec}[0]{\underline{x}}
\renewcommand{\vec}[1]{\underline{{#1}}}

\linespread{1.4} % 1.6 = double space, 1.3 = 1.5 space comment out for regular


\topmargin      0.0in
\headheight     0.0in
\headsep        0.0in
\oddsidemargin  0.0in
\evensidemargin 0.0in
\textheight     9.0in
\textwidth      6.5in


\title{{\bf Bayesian Zero-Inflated Decision Trees} \\ }
\author{ {\bf Lucas Roberts}  \\
Senior Data Scientist \\
The Hartford \\
{\small rlucas7@vt.edu}
}
\author{ {\bf Scotland Leman}  \\
Associate Professor \\
Virginia Tech \\
{\small rlucas7@vt.edu}
}

\date{\today}


\begin{document}
\maketitle

\abstract{
Wait until the end, when I think the draft is done to complete this section
}

\section{Introduction}
\label{ch:preliminaries}
This chapter provides an overview of the decision tree models that are commonly used in theoretical and applied settings. 

\textbf{1 \P on applications of count data with decision trees (motivation)}
Numerous applications exist in the statistical literature to model count data. Perhaps the most common is the Poisson distribution, this distribution has a rate parameter that dictates the frequency of an event per unit of measurement which may be a time, area, or some other unit of measurement. The ease with which counts are collected no doubt has influenced the ubiquity of count data applications. However, one common challenge when analyzing count data is that the occurrence of zero counts may occur more frequently than the Poisson model alone would suggest. In this scenario a zero inflation component is often used to model excess of the zero count over the Poisson distribution alone \cite{cite someone}. Similar distributions, called zero-modified exist for other count distributions \cite{cite loss distributions book}. 

While these models are inherently useful often one would like to relate the frequencies obtained from these zero-inflated models to various covariates, a zero-inflated Poisson (ZIP) regression is capable of modeling this relationship \cite{cite lambert}. Since the ZIP model was first proposed \cite{}, the ZIP model has been applied to numerous applications \textbf{proceed with cite list}. However, to the authors knowledge, ZIP models have not been applied to nematodes. The ZIP model has been applied to worms in soil applications in forestry \textbf{cite gudeta sileshi} but not to nematodes and not to agricultural applications. Moreover, the ZIP model requires sophistical statistical knowledge and is not yet a common model in the repertoire of most agricultural data analysts. Contrast this with decision trees which are simple and easily understood and interpreted with minimal technical training.  For this reason we developed the zero-inflated Poisson decision tree model. A similar model was proposed by \textbf{cite lee} and was fit using a maximum likelihood approach. This work was the inspiration for our work. We approached the tree fitting process from a Bayesian perspective and arrive at different resultant decision trees but with favorable conclusions. 



Bayesian approaches to building decision trees were not possible until advances in computing power and advances in sampling based approaches became available \cite{gelfand1990sampling}. While the work of Breiman et al \cite{breiman1984classification} always contained a Bayesian flavor, no probability measure over trees was ever proposed. That is until the groundbreaking works of Denison, Mallick, and Smith \cite{denison1998bayesian} and Chipman, George, and Mcculloch \cite{chipman1998bayesian}.

This manuscript proceeds as follows: 

Section \ref{sec:derivations} contains the derivations for the marginal likelihoods of the zero-inflated decision trees. 
Section \ref{sec:simulations} displays the results from simulate data. 
Section \ref{sec:twocasestudies} presents two case studies, one using the solder data from \textbf{? find the ref}  \cite{}  and the second uses data collected from a study of nematodes in a vineyard in Dobson county North Carolina. 
Section \ref{sec:discussion} puts the findings in context with other research, summarizes the results, and points towards future research. 

\section{The Model Details}
\label{sec:derivations}

\subsection{Bayesian Approaches}
\label{ch:p2sub_bayes}

This section describes the Bayesian approach to decision trees. The methods described in the previous chapter provided an algorithm to fit a decision tree using a greedy algorithm. Besides the observed error, there is nothing to describe the fit of the model, or to  provide a measure over decision trees. This section provides both of these quantities. We begin by defining the model and calculating necessary quantities for the algorithm. Furthermore, there is no explicit model selection, which will be the main contribution of this thesis.    

\subsubsection{The CGM approach}
We begin by defining notation and measures on each quantity of the tree. 
We assume the tree topology and split rules are conditionally independent. Based on fundamentals of probability we have the following relations:

\begin{align}
 \Pr(\mathcal{T}_i \vert \yvec, X ) &\propto \Pr(\mathcal{T}_i)\Pr(\yvec \vert \mathcal{T}_i, X)\\
  &\propto \Pr(\mathcal{T}_i)\int_{\Theta}\Pr(\yvec \vert \mathcal{T}_i, X, \theta)\pi(\theta)d\theta,
\end{align}
where $\Pr(\mathcal{T}_i)$, denotes the prior measure on trees and $\Pr(\yvec \vert \mathcal{T}_i, X)$ denotes the integrated likelihood of the tree. Finally, $\Pr(\yvec \vert \mathcal{T}_i, X, \theta)$, and$\pi(\theta)$, denote the tree likelihood and prior measure on node parameters respectively. This is the conditional decomposition defined by CGM \cite{chipman1998bayesian}. We now proceed to define the aspects of the model described in CGM's paper \cite{chipman1998bayesian}. 
The model has two main components, the tree $\mathcal{T}$ with $b$ terminal nodes, and the parameters in each terminal node $(\theta_1,\dots,\theta_b)$. 
The two main likelihoods in each terminal node are the normal and the multinomial, for continuous and categorical responses, respectively. 
Denote the responses in each terminal node as the vector  of vectors $Y\equiv (Y_1, \dots, Y_b)$. Then $Y_i = (y_{i1}, \dots y_{in_i})$, and the main relation is the independence breakdown 
\begin{equation}\label{eqn:lhood}
f(Y\vert \mathcal{T}, X, \theta) =\prod_{i=1}^b f(Y_i\vert \mathcal{T}, X, \theta_i) = \prod_{i=1}^b \prod_{j=1}^{n_i} f(y_{ij} \vert \mathcal{T}, X, \theta_i).
\end{equation}
The two likelihoods are now given by
\begin{equation}\label{eqn:normal_tree}
f(y_{ij} \vert \mathcal{T}, X, \theta_i) = N(\mu_i,\sigma_i),
\end{equation}
and the multinomial likelihood is
\begin{equation}\label{eqn:mult_lhood}
f(y_{i1}, \dots ,y_{in_i} \vert \mathcal{T}, X, \theta_i) = \prod_{j=1}^{n_i}\prod_{k=1}^K p_{ik}^{\mathds{1}(y_{ij}=k)}.
\end{equation}
In Equation \ref{eqn:mult_lhood}, $p_{ik}$ denotes the probability of being in category $k$ in terminal node $i$ and $\mathds{1}(A)$ denotes the indicator function for the set $A$. 

We now proceed to define the tree prior. We start with a tree consisting of a single node, the root node. We then imagine the tree growing by randomly choosing terminal nodes to split on. To grow a tree we must specify two functions, the growing function and the splitting function.  The splitting function is denoted as $p_{\text{split}}(\eta, \mathcal{T})$ and the rule function is defined as $p_{\text{rule}}(\rho \vert \eta, \mathcal{T})$. The rule function provides a criteria to determine which of the two children nodes observed data go into. If the observed covariate value is less than the rule value, then observations go into the left child node. Similarly, if the observed covariate value is greater than the rule value, then observations go into the right child node. Growing the tree consists of creating two new children from a terminal node and assigning a rule to the terminal node (now a parent of two terminal nodes). 

The probability measure on the potential splits of the tree is defined as
\begin{equation}\label{eqn:psplit}
p_{\text{split}}(\eta, \mathcal{T})=\alpha(1+d_{\eta})^{-\beta}, \ \ \alpha>0, \beta\geq 0,\\
\end{equation}
where $d_{\eta}$ denotes the depth of the node $\eta$ and $\alpha$, and $\beta$ are scalars. 
\begin{equation}\label{eqn:prule}
p_{\text{rule}}(\rho \vert \eta, \mathcal{T}) \propto \underbrace{\Pr(\text{split on covariate})}_{=(1)}\underbrace{\Pr(\text{split on a value given a covariate})}_{=(2)}. 
\end{equation}

Here CGM recommends using a uniform prior on (1), and splitting uniformly amongst the splitting values (in (2)) that do not result in an empty terminal node. While we choose the same proposal mechanism for quantities (1) and  (2), the main point of this manuscript is to examine and propose alternate specifications for Equation \ref{eqn:lhood} besides the normal and multinomial likelihoods specified in Equations \ref{eqn:normal_tree} and \ref{eqn:mult_lhood} respectively. 

\subsubsection{Integrated Likelihood}
We will now focus on the integrated likelihood, which is the quantity 

\begin{equation}\label{eqn:int_lhood}
\Pr(Y_i \vert \mathcal{T}, X) = \int_{\Theta}\Pr(Y_i \vert \mathcal{T}_i, X, \theta)\pi(\theta)d\theta.
\end{equation}
To evaluate the integral in Equation \ref{eqn:int_lhood} we must first define a prior, denoted $\pi(\theta)$, for the parameters in each terminal node. 
There are two possible priors for the case of the normal likelihood that will result in a conjugate prior/posterior. These are normals and normals and gammas, or normals and inverse gammas depending upon the given parametrization. 


\subsubsection{The Process Prior}

Assuming we have a closed form solution for the integral in Equation \ref{eqn:int_lhood}, we can use Bayes' rule to determine 
\begin{equation}\label{eqn:tree_post}
\Pr(\mathcal{T} \vert Y , X) \propto \Pr(Y \vert X ,\mathcal{T})\Pr(\mathcal{T}).
\end{equation} 
We now have an effective means of searching the posterior space over trees to determine the high posterior trees. We can do so by using the Metropolis-Hastings rule 

\begin{equation}\label{eqn:MHrule}
\mathcal{T}^{i+1} =\begin{cases}
\mathcal{T}^*, & \text{with probability}\ \alpha(\mathcal{T}^*, \mathcal{T}^i) = \text{min}\left(\frac{q(\mathcal{T}^*, \mathcal{T}^i)}{q(\mathcal{T}^i, \mathcal{T}^*)}\frac{\Pr(Y\vert X, \mathcal{T}^*)}{\Pr(Y\vert X,\mathcal{T}^i)}\frac{\Pr(\mathcal{T}^{*})}{\Pr(\mathcal{T}^i)},1 \right) \\
\mathcal{T}^{i}, & \text{with probability}\ 1-\alpha(\mathcal{T}^*, \mathcal{T}^i).
\end{cases} \end{equation}

To evaluate the normalization constant would require summing Equation \ref{eqn:tree_post} across all possible trees. This is a sum with $\mathcal{O}(nd\frac{4^h}{h^{3/2}})$ terms, with $h$ denoting the maximum height of the trees, $n$ denoting the number of observations, and $d$ denoting the number of covariates. This is an infeasible sum for most data sets, and for all data sets examined in this thesis. For the function $q(-\vert-)$, which is called the proposal function, we use $q$ to propose a new tree $\mathcal{T}^*$.   
In Equation \ref{eqn:MHrule}, $q(\mathcal{T}\vert\mathcal{T}^*)$ denotes proposing a new tree $\mathcal{T}^*$, starting from the current tree $\mathcal{T}$. 
 Our proposal mechanism is as follows:
  \begin{itemize}
 \item The grow step chooses at random one of the terminal nodes and proposes to append two new child nodes with a certain probability that depends on the tree depth, as stated in Equations \ref{eqn:psplit} and \ref{eqn:prule}.
 \item The prune step works in reverse of the grow. A terminal node is selected at random and that node and the node's sibling are pruned to the immediate parent of the two child nodes.
 \item The change step randomly picks an internal node and attempts to change the split rule at the node with that of another observation, possibly on a different covariate.
  \item The swap step randomly selects an internal node that is not the root node and proposes to swap the split rules of the parent-child pair. If both child nodes' split on the same covariate, then both children and the parent node's rules are swapped.
  \item The rotate step randomly chooses a left or right rotation move. Then this step randomly chooses an admissible internal node and rotates.
 \end{itemize}
  The rotate operation for binary trees was first introduced in Sleater and Tarjan \cite{sleator1985self} and was introduced into Bayesian decision trees in GL \cite{gramacy2008bayesian}. A good introduction and several practical uses of the rotate move can be found in Cormen, Lieserson, Rivest and Stein \cite{cormen2001introduction}. The proposal of Gramacy and Lee \cite{gramacy2008bayesian} only allows a rotate move for the specific case when a swap move is proposed and the parent child pair both split on the same covariate. We modify this and allow rotate to be a separate operation of the transition kernel and not a special swap move case. The proposal mechanism of CGM uses the grow, prune, change and swap moves only. We allow both swap moves and rotate moves in our proposal distribution.  
  
 The probability measure on the tree is defined as 
 
 \begin{equation}
 \Pr(\mathcal{T}) = \prod_{\eta \in \mathcal{T}} p_{\text{rule}}(\rho \vert \eta, \mathcal{T})p_{\text{split}}(\eta, \mathcal{T}).
\end{equation}

The probability measure on each split, here denoted $p_{\text{split}}(\eta, \mathcal{T})$, uses Equation \ref{eqn:psplit}. Similarly the measure on each rule, here denoted $p_{\text{rule}}(\rho \vert \eta, \mathcal{T})$, uses Equation \ref{eqn:prule}. 
All that is left to specify is the likelihood model in each node and the prior structure for the parameters in each node. This is done in the next subsection. 

\subsection{Node Likelihoods and Priors}

%
%We begin with the Gaussian likelihood and Gaussian prior model. We define the likelihood as 
%\begin{equation}\label{eqn:norm_lhood1}
%N[y_{ij} \vert \mu_i, \sigma^2].
%\end{equation}
%Also, we define the prior for $\mu_i$ as 
%\begin{equation}\label{eqn:norm_prior1}
%N[\mu_i\vert \bar{\mu}, \sigma^2 ].
%\end{equation}
%Furthermore, we define the prior for $\sigma^2$ as 
%\begin{equation}
%\text{Inv-Gamma}(\sigma^2\vert \alpha, \beta).
%\end{equation}
%All that remains is to evaluate the integral 
%\begin{equation}\label{eqn:int_model1}
%\prod_{i=1}^b \int_0^\infty \int_{-\infty}^{\infty} \prod_{j=1}^{n_i} N[y_{ij} \vert \mu_i, \sigma^2]N[\mu_i\vert \bar{\mu}, \sigma^2 ]\text{Inv-Gamma}(\sigma^2\vert \nu/2, \nu\lambda/2)d\mu_id\sigma^2.
%\end{equation}
%For this specific prior and likelihood we can explicitly calculate the marginal likelihood. Being able to marginalize the node parameters explicitly allows us to implement a Metropolis-Hastings algorithm without resorting to complicated, specialized algorithms, or numerical integrations. Straightforward analytic manipulations yield the solution to Equation \ref{eqn:int_model1} written here in Equation \ref{eqn:int_model1_soln}
%\begin{equation}\label{eqn:int_model1_soln}
%\frac{ca^{b/2}}{\prod_{i=1}^b\sqrt{n_i+a}}\times \left(\sum_{i=1}^b\left(\sum_{j=1}^{n_{i}}(y_{ij}-\bar{y}_i)^2\right)+ \frac{(\bar{y}_i -\bar{\mu})^2(n_ia)}{n_i+a} \right)^{-(\nu+n)/2}.
%\end{equation}
%
%Assuming instead that the variances might change from node to node, then the stated model is misspecified. Let us denote the variance in each node as $\sigma_i^2$ and keep all other notations from the stated model specification. Then the model is specified using 
%\begin{equation}\label{eqn:normal_likelihood_many_variance}
%N[y_{ij} \vert \mu_i, \sigma_i^2].
%\end{equation}
%Also, we define the prior for $\mu_i$ as 
%\begin{equation}\label{eqn:multi_variance_prior}
%N[\mu_i\vert \bar{\mu}, \sigma_i^2 ].
%\end{equation}
%Furthermore, we define the prior for the $\sigma_i^2$s as 
%\begin{equation}\label{eqn:sigma_priors}
%\text{Inv-Gamma}(\sigma_i^2\vert\nu/2, \nu\lambda/2)
%\end{equation}
% and now we evaluate the integral equation 
% \begin{equation}\label{eqn:int_model2}
%\prod_{i=1}^b \int_0^\infty \int_{-\infty}^{\infty} \prod_{j=1}^{n_i} N[y_{ij}\vert \vert \mu_i, \sigma_i^2]N[\mu_i\vert \bar{\mu}, \sigma_i^2 ]\text{Inv-Gamma}(\sigma_i^2\vert \nu/2, \nu\lambda/2)d\mu_id\sigma_i^2. 
%\end{equation}
%The result of computing the integrals in Equation \ref{eqn:int_model2} is 
%\begin{equation}\label{eqn:int_model3}
%\prod_{i=1}^b\pi^{n_i/2}(\lambda\nu)^{\nu/2}\sqrt{\frac{a}{n_i+a}}\frac{\Gamma((n_i+\nu)/2)}{\Gamma(\nu/2)}\times \left( \sum_{j=1}^{n_{i}}(y_{ij}-\bar{y}_i)^2+ \frac{(\bar{y}_i -\bar{\mu})^2(n_ia)}{n_i+a}+\nu\lambda  \right)^{(n_i+\nu)/2}.
%\end{equation}
%These are the two ``regression'' models  for Bayesian decision trees given in CGM \cite{chipman1998bayesian}. 
%
%The classification model discussed in CGM \cite{chipman1998bayesian} defines the likelihood, prior, and integrated likelihood as
%\begin{equation}
%y_{i1}, \dots, y_{in_i} \vert \mathcal{T} \sim \text{Multinomial}(Y_i \vert \vec{n}, \vec{p}),
%\end{equation} 
%\begin{equation}
%\vec{p} \vert \mathcal{T} \sim \text{Dirichlet}(\vec{p}\vert \vec{\alpha}), 
%\end{equation} 
%and
%\begin{equation}\label{eqn:int_model4}
%\Pr(Y\vert \mathcal{T}, X)=\left(\frac{\Gamma(\sum_{k=1}^K\alpha_k)}{\prod_{k=1}^K\Gamma(\alpha_k)} \right)^b \prod_{i=1}^b\left( \frac{\prod_{k=1}^K\Gamma(n_{ik}+\alpha_k)}{\Gamma(n_i +\sum_{k=1}^K\alpha_k)} \right),
%\end{equation} 
%respectively. 
% 
CGM discuss three models. Two of the models use Gaussian priors and Gaussian likelihoods and one of the models uses a Dirichlet prior and a multinomial likelihood. The two Gaussian models differ in that one has a single variance and the other has a different variance for each node. As noted by Lee \cite{lee2006decision}, in a greedy optimization context, sometimes the data suggest a different model than either a Gaussian or a multinomial-Dirichlet. If the experiment suggests analyzing data using an alternate model, the Bayesian context easily handles these alterations, once the corresponding likelihood and prior are specified. In the case of Lee \cite{lee2006decision}, a zero inflated poisson (ZIP) model was proposed to analyze the solder data. Our Bayesian model can easily handle extensions such as this and also permits covariate selection, provided the integrated likelihood is available in closed form.

If we wanted to model the data using a different data generating process, for example a ZIP. We could do so by specifying a different likelihood, prior, and computing the integrated likelihood. For a ZIP model this is possible using gamma priors for the rate ($\lambda$) parameter and beta priors for zero inflation component ($\phi$). 
 
 \subsubsection{A Bayesian Zero Inflated Poisson Model}
 %\textbf{Write out the details of this model here and cite applied statistics paper which uses a greedy approach.}
 
 Lee and Jin \cite{lee2006decision} reconsidered impurity functions in light of the connection to likelihood functions. Lee and Jin \cite{lee2006decision} proposed to use likelihood functions instead of impurity functions that model the data generating process. Towards this end they considered the soldering data from Chambers and Hastie \cite{chambers1992statistical}. The response of interest in this case is a collection of counts on manufactured circuit boards. This response has many zero's and Lee and Jin \cite{lee2006decision} propose using a zero inflated (ZIP) poisson likelihood to model the measured counts. Lee and Jin \cite{lee2006decision} optimized using a greedy algorithm and they found the fit and holdout prediction to be better using the ZIP model in each terminal node. If we are to use a Bayesian approach to this problem we need to define the likelihood, the prior, and the integrated likelihood. We now define these three quantities.
 
 The likelihood for a single observation is 
 \begin{equation}
 f(y\vert \lambda, \phi) \propto \mathds{1}(y=0)\left(\phi + (1-\phi)\exp{(-\lambda)}\right) + \mathds{1}(y>0)\left(\exp{(-\lambda)}\frac{\lambda^y}{y!}\right).
 \end{equation}
 The priors for $\lambda$ and $\phi$ are
 \begin{equation}
 \pi(\phi, \lambda)\propto \underbrace{\frac{\phi^{\alpha-1}(1-\phi)^{\beta-1}\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}}_{=\text{A beta prior }} \times \underbrace{\frac{ \lambda^{\alpha_{\lambda}-1}\exp{(-\lambda/\beta_{\lambda})} }{\Gamma(\alpha_{\lambda})\beta_{\lambda}^{\alpha_{\lambda}}}}_{=\text{A gamma prior}}.
 \end{equation}

 Now we need to calculate the integrated likelihood, which means we must evaluate 
 \begin{align} \hspace{-.9in}
 &\int_0^1\int_0^\infty \left(\mathds{1}(y=0)\left(\phi + (1-\phi)\exp{(-\lambda)}\right) + \mathds{1}(y>0)\left(\exp{(-\lambda)}\frac{\lambda^y}{y!}\right)\right) \\ \nonumber
 & \frac{\phi^{\alpha-1}(1-\phi)^{\beta-1}\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \times \frac{ \lambda^{\alpha_{\lambda}-1}\exp{(-\lambda/\beta_{\lambda})} }{\Gamma(\alpha_{\lambda})\beta_{\lambda}^{\alpha_{\lambda}}}d\lambda d\phi.
 \end{align}
 
 Let $j$ index the observed zero counts. Furthermore, let $\bar{y}_{+}$ denote the average of the non-zero counts and $n_0$ and $n_+$ denote the number of zeros and non-zeros in the data respectively. 
 Now we assume that the observations are $i.i.d.$ and simple calculations lead to the conclusion that
 
 \begin{align}\label{eqn:zip_int_lhood}\hspace{-.7in}
 \Pr(Y\vert X, \mathcal{T})& = \left[\sum_{j=0}^{n_0} {n_0 \choose j}\frac{\Gamma(\alpha+\beta)\Gamma(\alpha+j)\Gamma(n_0+\beta-j)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\alpha+\beta+n_0)}\left(\frac{n_0-j+\beta_{\lambda}^{-1}}{\beta_{\lambda}}\right)^{\alpha_{\lambda}} \right] \\ \nonumber
 & + \frac{\Gamma(\alpha_{\lambda}+n_+\bar{y}_+)}{\Gamma(\alpha_{\lambda})\beta_{\lambda}^{\alpha_{\lambda}}}\left(n_+ + 1/\beta_{\lambda} \right)^{\alpha_{\lambda}+n_+\bar{y}_+}.
\end{align}

\subsection{ZIP Derivations}

In this section we provide the derivation for the integrated likelihood for the Bayes ZIP\newabbrev{abbrev:ZIP} (zero inflated poisson) tree model. Now let us define some notation: $j$ will index either all observations or only the observed zero count observations if the upper limit is $n_0$ then $j$ will index observed zero counts only, if the upper index limit is $n$ then all observations are indexed. Also $j^{\prime}$ will index the non-zero observations. The total number of non-zero observations is denoted $n_+$, so that $n_++n_0=n$. Finally let $\bar{y}_{i+}$ denote the sample mean of the non-zero count observations in terminal node $i$. 

\begin{align*}
\Pr(Y \vert X, \mathcal{T}) &= \prod_{i=1}^b\int_0^1\int_0^\infty\prod_{j=1}^{n_i}\left[\mathds{1}[y_{ij}=0](\phi+(1-\phi)\exp{(-\lambda)})+\mathds{1}[y_{ij}>0]\frac{\exp{(-\lambda)\lambda^{y_{ij}}}}{y_{ij}!} \right]\pi(\phi_i,\lambda_i )d\lambda_id\phi_i\\
&=\prod_{i=1}^b \int_0^1\int_0^\infty\underbrace{\prod_{j=1}^{n_0}(\phi + (1-\phi)\exp{(-\lambda)})\pi(\phi_i,\lambda_i )d\lambda_id\phi_i}_{=(1)}\\ 
&+ \prod_{i=1}^b \int_0^1\int_0^\infty \underbrace{\prod_{j^\prime=1}^{n_+}\frac{\exp{(-\lambda)}\lambda^{y_{ij^\prime}} }{y_{ij^\prime}!}\pi(\phi_i,\lambda_i )d\lambda_id\phi_i}_{=(2)}.\\ 
\end{align*} 

We will first tackle $(1)$, then tackle $(2)$. 

\begin{align*}
(1)&= \int_0^1\int_0^\infty\prod_{j=1}^{n_0}(\phi + (1-\phi)\exp{(-\lambda)})\pi(\phi_i,\lambda_i )d\lambda_id\phi_i \\
&= \int_0^1\int_0^\infty(\phi + (1-\phi)\exp{(-\lambda)})^{n_0}\pi(\phi_i,\lambda_i )d\lambda_id\phi_i \\
&= \int_0^1\int_0^\infty\sum_{j=1}^{n_0}{n_0\choose j}\phi^{j}(1-\phi)^{n_0-j}\exp{(-(n_0-j)\lambda)})\pi(\phi_i)\pi(\lambda_i )d\lambda_id\phi_i. \\
\end{align*}
 
 Now we take $\pi(\phi_i)$ to be a beta($\alpha, \beta$) prior and $\pi(\lambda_i)$ to be a gamma($\alpha_{\lambda}, \beta_{\lambda}$) prior. This simplifies matters greatly. 
 
 \begin{align*}
 & \int_0^1\int_0^\infty\sum_{j=1}^{n_0}{n_0\choose j}\phi^{j}(1-\phi)^{n_0-j}\exp{(-(n_0-j)\lambda)})\frac{\Gamma(\alpha+\beta)\phi^{\alpha-1}(1-\phi)^{\beta-1}}{\Gamma(\alpha)\Gamma(\beta)}\frac{\lambda^{\alpha_{\lambda}-1}\exp{(-\lambda/\beta_{\lambda})}}{\Gamma(\alpha_{\lambda})\beta_{\lambda}^{\alpha_{\lambda}}} d\lambda_id\phi_i \\
&=\sum_{j=1}^{n_0}{n_0\choose j}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\alpha_{\lambda})\beta_{\lambda}^{\alpha_{\lambda}}} \underbrace{\int_0^1\phi^{j+\alpha-1}(1-\phi)^{\beta+n_0-j-1}d\phi_i}_{\text{a beta kernel}}  \underbrace{\int_0^\infty \lambda^{\alpha_{\lambda}-1} \exp{(-(n_0-j+\beta_{\lambda}^{-1})\lambda)} d\lambda_i}_{\text{a gamma kernel}}\\
&=\underbrace{\sum_{j=1}^{n_0}{n_0\choose j}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\alpha_{\lambda})\beta_{\lambda}^{\alpha_{\lambda}}} \frac{\Gamma(\alpha+j)\Gamma(\beta+n_0-j)\Gamma(\alpha_{\lambda})}{\Gamma(\alpha+\beta+n_0)}(n_0-j+\beta_{\lambda}^{-1})^{\alpha_{\lambda}}}_{=(1)}.
\end{align*} 
Now with the first piece simplified we move on to piece $(2)$. 
 
 \begin{align*}
 (2) &= \int_0^1\int_0^\infty\prod_{j^\prime=1}^{n_+}\frac{\exp{(-\lambda)}\lambda^{y_{ij^\prime}} }{y_{ij^\prime}!}\pi(\phi_i,\lambda_i )d\lambda_id\phi_i \\
 &= \int_0^\infty \prod_{j^\prime=1}^{n_+}\frac{\exp{(-\lambda)}\lambda^{y_{ij^\prime}} }{y_{ij^\prime}!}\pi(\lambda_i )d\lambda_i \\
 &= \int_0^\infty\frac{\exp{(-n_+\lambda)}\lambda^{n_+\bar{y}_{i+}} }{\prod_{j^\prime=1}^{n_+}y_{ij^\prime}!}\pi(\lambda_i )d\lambda_i \\
 &= \int_0^\infty\frac{\exp{(-n_+\lambda)}\lambda^{n_+\bar{y}_{i+}} }{\prod_{j^\prime=1}^{n_+}y_{ij^\prime}!}\frac{\lambda^{\alpha_{\lambda}-1}\exp{(-\lambda/\beta_{\lambda})}}{\Gamma(\alpha_{\lambda})}d\lambda_i \\
 &= \frac{\int_0^\infty\exp{(-(n_+ +\beta_{\lambda}^{-1})\lambda)}\lambda^{n_+\bar{y}_{i+} +\alpha_{\lambda}-1} d\lambda_i}{\Gamma(\alpha_{\lambda})\prod_{j^\prime=1}^{n_+}y_{ij^\prime}!} \\
 &= \underbrace{\frac{\Gamma(n_+\bar{y}_{i+}+\alpha_{\lambda})(n_+ +\beta_{\lambda}^{-1})^{n_+\bar{y}_{i+}+\alpha_{\lambda}}}{\Gamma(\alpha_{\lambda})\prod_{j^\prime=1}^{n_+}y_{ij^\prime}!}}_{=(2)}. \\
 \end{align*}
 
 And the result is shown. 

\section{Simulations}
\label{sec:simulations}

\section{Two Case Studies}
\label{sec:twocasestudies}

\subsection{The Solder Data}
In this section we analyze the data from the soldering experiment from Chambers and Hastie \cite{chambers1991statistical}. The data contains 900 rows and 6 columns. The data was collected from an industrial experiment of manufactured circuit boards and the number of soldering defects present on the circuit boards. These data were analyzed by Lambert and were the motivating application behind the development of the zero-inflated Poisson regression model \cite{lambert1992zero}. The data are available in the R package \emph{faraway} or you may use a balanced subset of 720 observations available in the R package \emph{rpart} \cite{2015rpart}. We use the full data from the \emph{faraway} package \cite{faraway2016Rpack}. The columns in the data are:
Opening, Solder, Mask, PadType, Panel, and skips. The skips are the response variable of interest which represent the number of skips in the soldering of the circuit board. The Panel variable is a numeric variable taking on the values 1, 2, and 3. The remaining variables are all categorical with the number of levels for each factor ranging from 2 levels to 10 levels. 

\subsection{The Nematode data}

Nematodes are a form of microscopic worms. While some forms of nematodes are harmless many are vectors for disease for various plant species. In particular, ring nematodes  are a vector for disease in grapevines. While nematodes are common in soil they are not ubiquitous and thus testing the soil for nematodes is an important aspect of viticulture. Moreover, a small amount of nematodes may be acceptable while the absence of nematodes is ideal. One challenge researchers often face when dealing with analyzing nematode data is the high occurrence of zero nematodes in soil samples. While this is good from the viticulturist's perspective this complicates the analysis required because this makes the use of standard, Gaussian based statistical analyses incorrect. Moreover using Gaussian based models can lead to erroneous conclusions. 

An additional difficulty placed on the analyst is that the ZIP regression model is non-standard and requires interpreting two regression models, one for the zero inflation component and one for the Poisson frequency component. Often a single regression model is challenging enough to explain and two seems needlessly complex. Contrast this with a decision tree which is mostly self explanatory and needs less statistical explanation than a regression model. 

We analyze data taken from field experiments conducted over several years at a vineyard in Dobson county North Carolina. While other aspects of the statistical analyses have been reported elsewhere \cite{} the analysis of the nematode data via zero-inflated models has yet to be reported. We initially built ZIP regression models but found them to be complex relative to decision trees and opted to pursue the analysis via decision trees for their easy interpretation and the subsequent increased likelihood of their being used by viticulturists. 

\P describe the experiment. This \P may be lifted from one of the AJEV papers. 

We analyzed the nematode data on ringworms to illustrate the use of the technique. The ultimate goal being to describe the relationship between various viticultural factors and the prevalence and/or the presence of nematodes. We compare these results with those from a Gaussian decision tree model and a ZIP regression model for comparison in Table \ref{}. 
Table \ref{} displays the mean squared error of each model as well as the negative log-likelihood of each model. 

\section{Discussion}\label{sec:discussion}



\bibliographystyle{plain}   % this means that the order of references
			    % is dtermined by the order in which the
			    % \cite and \nocite commands appear
\bibliography{tree_biblio, tree_biblio1} 

\end{document}